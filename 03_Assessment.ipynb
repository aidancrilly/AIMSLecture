{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDTxf3A1gT/ql1iQvbIfAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/AIMSLecture/blob/main/03_Assessment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assessment\n",
        "\n",
        "## Quiz questions\n",
        "\n",
        "1) Consider the following system of equations:\n",
        "\n",
        "$$\n",
        "  \\frac{d^2}{dt^2}\\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} -\\lambda & \\omega \\\\ \\omega & -λ \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} \\ , \\ x(0) = x_0 \\ , \\ y(0) = y_0\n",
        "$$\n",
        "\n",
        "(a) How many parameters would be solved for in the forward pass?\n",
        "\n",
        "(b) How many parameters would be solved for in the reverse/adjoint pass?\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "etsMPOUu2Lxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "XypOj9XaOcXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "2) For the transport equation in exercise 2, consider the situation where the dimensionality is reduced further and one is only interested in the differential equation for $N$ as defined as:\n",
        "\n",
        "$$\n",
        " N(t) = \\int_{-\\infty}^{+\\infty} \\phi(x,t) dx\n",
        "$$\n",
        "\n",
        "Does this reduced model contain any unknowns which would need to be learnt? If so, what are they?\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "8CpVqNJ2Oegg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "FhUX9g6ZOnuH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "3) (Multiple Choice, choose all that apply) Which of the following are valid finite differences of a first order derivative, $df/dx$:\n",
        "\n",
        "(a) $\\frac{f_{i+1}-f_{i-1}}{\\Delta x}$\n",
        "\n",
        "(b) $\\frac{f_{i+1}-2f_{i}+f_{i-1}}{\\Delta x^2}$\n",
        "\n",
        "(c) $\\frac{f_{i+1}-f_{i}}{\\Delta x}$\n",
        "\n",
        "(d) $\\frac{f_{i}-f_{i-1}}{\\Delta x}$\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "5zo0__P7Ooyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "OE5ECNKlOtsm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "4) From the first exercise, we solved a simple decay problem using the forward Euler method:\n",
        "\n",
        "$$ y^{i+1} = \\left( 1 - \\frac{dt}{\\tau} \\right) y^{i} $$\n",
        "\n",
        "This is known as a 'time-explicit' method.\n",
        "\n",
        "We could also take the backwards finite difference of the time derivative, arriving at backwards Euler (a 'time-implicit' method):\n",
        "\n",
        "$$ y^{i+1} = \\frac{y^{i}}{1 + \\frac{dt}{\\tau}} $$\n",
        "\n",
        "(a) Which method gives a better answer for a single long time step ($dt \\gg \\tau$)?\n",
        "\n",
        "(b) As in the lecture, we can compute the gradient w.r.t. $\\tau$ across a single time step. For the forward Euler step, this was:\n",
        "\n",
        "$$ \\frac{d y^{i+1}}{d \\tau} = \\frac{dt}{\\tau^2}y^{i} + \\left( 1 - \\frac{dt}{\\tau} \\right) \\frac{d y^{i}}{d \\tau} $$\n",
        "\n",
        "Perform the same for the backwards Euler.\n",
        "\n",
        "***\n",
        "\n"
      ],
      "metadata": {
        "id": "6enss3TLOu1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "qL7XZZLsOzT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "5) Displayed are two computational graphs, one of which is the AD of the other:\n",
        "\n",
        "Graph 1:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nA-WEbd08HmctWCY0I6Buv4mOhvSo80b)\n",
        "\n",
        "Graph 2:\n",
        "![](https://drive.google.com/uc?export=view&id=1WQQ4SpPpMXC3mUTnuaRtivV54Q-exqwH)\n",
        "\n",
        "\n",
        "a) What are the shapes of the inputs and outputs of these graphs?\n",
        "\n",
        "b) (Multiple Choice, choose one answer only) Which of the following is the base function:\n",
        "\n",
        "(i) $f(\\underline{x},\\underline{y}) = \\underline{y}^{T} \\cdot \\underline{x}$\n",
        "\n",
        "(ii) $f(\\underline{x},\\underline{\\underline{A}}) = \\underline{\\underline{A}}\\cdot \\underline{x}$\n",
        "\n",
        "(iii) $f(\\underline{x},\\underline{\\underline{A}}) = \\underline{x}^{T} \\cdot \\underline{\\underline{A}}\\cdot \\underline{x}$\n",
        "\n",
        "c) Which graph displays the AD of this function?\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "IvF7XtVWO0lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "MOM6uY4QO40C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "6) Gradients are also useful for solving other problems in scientific computing beyond optimisation. One equation is root-finding.\n",
        "\n",
        "In root-finding, we want to find a zero of a function, for this question lets consider just a simple 1D function f(x):\n",
        "\n",
        "$$ f(x) = 0 $$\n",
        "\n",
        "We note that the optimisation problem can be recast as a root-finding problem as the optimum of $g(x)$ is defined as:\n",
        "\n",
        "$$ g'(x) = 0 $$\n",
        "\n",
        "Therefore, if we take $f(x) \\equiv g'(x)$ and root-find $f(x)$, this is equivalent to optimising $g(x)$.\n",
        "\n",
        "The Newton-Raphson method is an iterative method that uses the local tangent to take a step towards the root:\n",
        "\n",
        "$$ x_{n+1} = x_{n} - \\frac{f(x_n)}{f'(x_{n})}$$\n",
        "\n",
        "(a) (Multiple Choice, choose one answer only) This differs from a gradient descent method, which of the following denotes the gradient descent method for the analagous optimisation problem of $g(x)$:\n",
        "\n",
        "(i) $ x_{n+1} = x_{n} + \\alpha f(x_{n+1})$\n",
        "\n",
        "(ii) $ x_{n+1} = x_{n} - \\alpha f(x_{n})$\n",
        "\n",
        "(iii) $ x_{n+1} = x_{n} - \\alpha f'(x_n)$\n",
        "\n",
        "(b) Consider the following function (known as the Forrester function), where $x \\in [0,1]$:\n",
        "\n",
        "$$ g(x) = (6x - 2)^2 \\sin(12x-4) $$\n",
        "\n",
        "Below is some code written to optimise it via gradient descent and Newton-Raphson.\n",
        "\n",
        "```python3\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def g(x):\n",
        "  # Forrester function\n",
        "  return (6*x-2)**2*jnp.sin(12*x-4)\n",
        "\n",
        "# Starting point\n",
        "x = 0.0\n",
        "\n",
        "def f(x):\n",
        "  return ???\n",
        "\n",
        "def fprime(x):\n",
        "  return ???\n",
        "\n",
        "# Gradient Descent (GD) step\n",
        "x_GD = ???\n",
        "\n",
        "# Newton-Raphson step\n",
        "x_NR = x - f(x)/fprime(x)\n",
        "```\n",
        "\n",
        "(Multiple Choice, choose one answer only) What are the missing lines:\n",
        "\n",
        "(i)\n",
        "```python3\n",
        "def f(x):\n",
        "  return jax.grad(g)\n",
        "\n",
        "def fprime(x):\n",
        "  return jax.grad(f)\n",
        "\n",
        "# Gradient Descent (GD) step\n",
        "x_GD = x + 0.001*f(x)\n",
        "\n",
        "# Newton-Raphson step\n",
        "x_NR = x - f(x)/fprime(x)\n",
        "```\n",
        "\n",
        "(ii)\n",
        "```python3\n",
        "def f(x):\n",
        "  return jax.grad(g)\n",
        "\n",
        "def fprime(x):\n",
        "  return jax.grad(f)\n",
        "\n",
        "# Gradient Descent (GD) step\n",
        "x_GD = x - 0.001*f(x)\n",
        "\n",
        "# Newton-Raphson step\n",
        "x_NR = x - f(x)/fprime(x)\n",
        "```\n",
        "\n",
        "(iii)\n",
        "```python3\n",
        "def f(x):\n",
        "  return jax.grad(g)(x)\n",
        "\n",
        "def fprime(x):\n",
        "  return jax.grad(f)(x)\n",
        "\n",
        "# Gradient Descent (GD) step\n",
        "x_GD = x - 0.001*f(x)\n",
        "\n",
        "# Newton-Raphson step\n",
        "x_NR = x - f(x)/fprime(x)\n",
        "```\n",
        "\n",
        "(c) Can you identify a starting point, $x^*$, which will cause issue for both gradient descent and Newton-Raphson for the Forrester function? What is the origin of the issue?\n",
        "\n",
        "***\n"
      ],
      "metadata": {
        "id": "xyDBiPgqO5tp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "gqCGpYZkO_oK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***\n",
        "\n",
        "7) (Advanced) Consider the following differential equation (known as a neural differential equation):\n",
        "\n",
        "$$\n",
        "\\frac{d y}{d t} = \\mathcal{N}_\\theta(y)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{N}_\\theta(y)$ is a neural network and $y$ is a 1D vector.\n",
        "\n",
        "(a) (Multiple Choice, choose one answer only) If we use constant Euler time stepping ($\\Delta t$ = const.) to solve this equation, what class of neural network does this resemble:\n",
        "\n",
        "$$\n",
        "  y_{t+1} = y_{t} + \\Delta t \\mathcal{N}_\\theta(y_{t})\n",
        "$$\n",
        "\n",
        "(i) Convolutional Neural Network\n",
        "\n",
        "(ii) Recurrent Neural Network\n",
        "\n",
        "(iii) Feedforward Neural Network\n",
        "\n",
        "(b) (True/False) If the neural network $\\mathcal{N}_\\theta(y)$ contains a single convolutional layer, kernel size = 3 with 1 input channel ($y$) and 1 output channel ($dy/dt$), one can learn a heat equation for y:\n",
        "\n",
        "$$\n",
        "\\frac{d y}{d t} = D \\frac{d^2 y}{d x^2}\n",
        "$$\n",
        "\n",
        "***"
      ],
      "metadata": {
        "id": "iv8acawSO_q2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "vmZIGriuPEkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Coding exercise\n",
        "Scenario: we are presented with data that gives the abundance of 30 different materials as a function of time. These materials are part of a reaction network which involves only simple decays. There are 30 datasets each having at time = 0 only one of the materials present. A mathematical model of this system is therefore:\n",
        "\n",
        "$$\n",
        "\\frac{d \\underline{f}}{dt} = \\underline{\\underline{D}} \\cdot \\underline{f} \\ , \\\\\\underline{f}_0(t=0) = [1,0,...,0]\\ , \\ \\underline{f}_1(t=0) = [0,1,...,0] \\ \\mathrm{etc.}\n",
        "$$\n",
        "\n",
        "Where the matrix $\\underline{\\underline{D}}$ give the various rates of decay between the materials. To properly represent the physics, $\\underline{\\underline{D}}$ must have the following properties:\n",
        "\n",
        "1. $\\underline{\\underline{D}}$ has semi-positive ($\\geq$ 0) off-diagonal elements.\n",
        "2. The diagnonal elements obey the following relation $D_{ii} = -\\sum_{j \\ne i} D_{j,i}$\n",
        "\n",
        "\n",
        "Unfortunately we do not know any of the rate coefficents a-priori. Your task is the complete the following code below in order to learn from the data the elements of $\\underline{\\underline{D}}$.\n",
        "\n",
        "Hints:\n",
        "\n",
        "1. The off-diagonal elements in $\\underline{\\underline{D}}$ are ≲ 1\n",
        "2. The fraction of different materials can vary by orders of magnitude, however we aim to minimise relative error. Choose a loss function which can accomplish this - typical MSE might not be the best choice."
      ],
      "metadata": {
        "id": "s5ASxqcR2a20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import ***\n",
        "\n",
        "def reaction_equation_dydt(t,y,args):\n",
        "  \"\"\"\n",
        "  To be completed, N.B. reaction matrix D is in args\n",
        "  \"\"\"\n",
        "  return res\n",
        "\n",
        "def construct_D_matrix(D_coefs,N):\n",
        "  \"\"\"\n",
        "  Don't alter this code!\n",
        "\n",
        "  D_coefs : 1D JAX array of length N**2-N\n",
        "  N : number of materials\n",
        "\n",
        "  This code creates the D matrix based on the coefficients\n",
        "\n",
        "  Semi-positivity is enforced by squaring the coefficients\n",
        "  The diagonal is constructed from the off-diagonal elements\n",
        "  \"\"\"\n",
        "  # Make semi-positive using squaring operation\n",
        "  D_semipos = D_coefs**2\n",
        "\n",
        "  # Create indices for the upper triangular part (excluding diagonal)\n",
        "  rows, cols = jnp.triu_indices(N, k=1)\n",
        "\n",
        "  matrix = jnp.zeros((N,N))\n",
        "\n",
        "  # Fill the upper triangle\n",
        "  matrix = matrix.at[rows, cols].set(D_semipos[:len(rows)])\n",
        "\n",
        "  # Fill the lower triangle\n",
        "  matrix = matrix.at[cols, rows].set(D_semipos[len(rows):])\n",
        "\n",
        "  diags = jnp.arange(N)\n",
        "  matrix = matrix.at[diags,diags].set(-jnp.sum(matrix,axis=0))\n",
        "\n",
        "  return matrix"
      ],
      "metadata": {
        "id": "Wvjee3IW2OMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "training_data = jnp.load('fs_truth.npy')\n",
        "_,Nt,Nreac = training_data.shape\n",
        "\n",
        "t0 = 0.0\n",
        "t1 = 5.0\n",
        "dt0 = 0.01\n",
        "\n",
        "ireacs = jnp.arange(Nreac)\n",
        "\n",
        "# At what time points you want to save the solution\n",
        "saveat = diffrax.SaveAt(ts=jnp.linspace(t0,t1,Nt))\n",
        "\n",
        "def solve_reaction_equation(D_coeffs,Nreac,ireac0):\n",
        "  \"\"\"\n",
        "  To be completed:\n",
        "\n",
        "   - Set the correct initial conditions\n",
        "   - Complete the diffrax diffeqsolve\n",
        "\n",
        "  D_coeffs : 1D JAX array of length N**2-N, D matrix coefficients\n",
        "  Nreac : number of materials\n",
        "  ireac0 : index of reaction which has a non-zero initial condition\n",
        "\n",
        "  \"\"\"\n",
        "  y0 =\n",
        "  args = {'D' : construct_D_matrix(D_coeffs,Nreac)}\n",
        "  sol = diffrax.diffeqsolve(...)\n",
        "\n",
        "  return sol.ys\n",
        "\n",
        "# Maps solve_reaction_equation over the different initial conditions\n",
        "# Note that D_coeffs,Nreac are not mapped over as these are the same for each set of initial conditions\n",
        "# The output of this vmapped_solve will be shape (Nreac,Nt,Nreac), the same as the training data\n",
        "vmapped_solve = lambda D,N : jax.vmap(solve_reaction_equation,\n",
        "                                      in_axes=(None,None,0))(D,N,ireacs)"
      ],
      "metadata": {
        "id": "fbYlrqVb2Qfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reaction_loss(D_coeffs,Nreac,data):\n",
        "  fs = vmapped_solve(D_coeffs,Nreac)\n",
        "  loss = # Error between fs and data\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "xKRKrtD92RxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}