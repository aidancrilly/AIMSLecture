{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/AIMSLecture/blob/main/2026/Project_DPNDE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f1a52e5",
      "metadata": {
        "id": "9f1a52e5"
      },
      "source": [
        "# Neural Ordinary Differential Equations for Chaotic Systems\n",
        "\n",
        "The double pendulum is a physical system which exhibits chaotic behaviour (for some initial conditions). As the name suggests, it consists of two masses connected by fixed rods which swing under gravity.\n",
        "\n",
        "Below is a diagram of the double pendulum system:\n",
        "\n",
        "![](https://dassencio.org/assets/double-pendulum.483b215e.svg)\n",
        "\n",
        "We will consider the simplified system where $m_1 = m_2$ and $l_1 = l_2$.\n",
        "\n",
        "The dynamics of this system are described by the equations of motion:\n",
        "\n",
        "$$ \\mathbf{y} = \\begin{bmatrix} \\theta_1 \\\\ \\theta_2 \\\\ \\dot{\\theta}_1 \\\\ \\dot{\\theta}_2 \\end{bmatrix} \\ \\ , \\ \\ \\frac{d \\mathbf{y}}{dt} = \\begin{bmatrix} \\dot{\\theta}_1 \\\\ \\dot{\\theta}_2 \\\\ h_1(\\theta_1,\\theta_2,\\dot{\\theta}_1,\\dot{\\theta}_2) \\\\ h_2(\\theta_1,\\theta_2,\\dot{\\theta}_1,\\dot{\\theta}_2)  \\end{bmatrix} $$\n",
        "\n",
        "Where $\\theta_1$ and $\\theta_2$ are the angles of the pendula (see diagram) and dotted variables denote time derivatives.\n",
        "\n",
        "Note $h_1$ and $h_2$ are the non-linear angular force functions, which we will learn via NDE methods.\n",
        "\n",
        "## Problem statement\n",
        "\n",
        "Your task is to:\n",
        "\n",
        " - Write a neural differential equation system for the double pendulum system.\n",
        " - Train this model on data produced by direct numerical solutions to the true double pendulum system.\n",
        " - Understand the training and behaviour of NDEs and investigate their behaviour for chaotic systems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92e8331",
      "metadata": {
        "id": "f92e8331"
      },
      "outputs": [],
      "source": [
        "!pip install equinox optax diffrax\n",
        "!git clone https://github.com/aidancrilly/AIMSLecture.git\n",
        "\n",
        "import sys\n",
        "sys.path.append('./AIMSLecture/2026/')\n",
        "\n",
        "import jax\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "import diffrax\n",
        "import time\n",
        "import copy\n",
        "import optax\n",
        "import equinox as eqx\n",
        "\n",
        "# Note this should be found via the git clone + sys.path.append above\n",
        "from DoublePendulumSolver import DoublePendulum as DPS\n",
        "from DoublePendulumSolver import g\n",
        "from DoublePendulumSolver import ODE_kwargs as NDE_args\n",
        "\n",
        "L = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf49c0c1",
      "metadata": {
        "id": "cf49c0c1"
      },
      "source": [
        "### Training data creation\n",
        "\n",
        "Here we create training data for the double pendulum problem, running Nsamples simulations for t between 0 and tmax. We start the pendula from rest, $\\omega$ range = [0,0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d520d5a",
      "metadata": {
        "id": "0d520d5a"
      },
      "outputs": [],
      "source": [
        "def generate_training_data(Nsamples, theta_range, omega_range, tmax, L = L, Nt = 1000, key = jax.random.PRNGKey(42)):\n",
        "    key,subkey = jax.random.split(key)\n",
        "    y0s = jax.random.uniform(subkey,shape=(Nsamples,4))\n",
        "    y0s = y0s.at[:,:2].set(theta_range[0] + (theta_range[1]-theta_range[0])*y0s[:,:2])\n",
        "    y0s = y0s.at[:,2:].set(omega_range[0] + (omega_range[1]-omega_range[0])*y0s[:,2:])\n",
        "\n",
        "    ts = jnp.linspace(0.0,tmax,Nt)\n",
        "    DP = DPS(ts=ts)\n",
        "\n",
        "    init_cond = {\n",
        "        'theta1' : y0s[:,0],\n",
        "        'theta2' : y0s[:,1],\n",
        "        'theta1dot' : y0s[:,2],\n",
        "        'theta2dot' : y0s[:,3],\n",
        "        }\n",
        "\n",
        "    args = {'L' : L}\n",
        "\n",
        "    ys = jax.jit(jax.vmap(DP.__call__,in_axes=[0,None]))(init_cond, args)\n",
        "    return ts, ys\n",
        "\n",
        "# No need to change these\n",
        "theta_range = [-jnp.pi/2.0,jnp.pi/2.0]\n",
        "omega_range = [0.0,0.0]\n",
        "Nsamples = 1200\n",
        "tmax = 10.0\n",
        "\n",
        "ts, full_ys = generate_training_data(Nsamples, theta_range, omega_range, tmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae830521",
      "metadata": {
        "id": "ae830521"
      },
      "source": [
        "These data contain chaotic trajectories (we shall discuss later), for training purposes we wish to remove these, to do this we do a crude cut of data based on the range of angles in the trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3586cb3",
      "metadata": {
        "id": "b3586cb3"
      },
      "outputs": [],
      "source": [
        "def split_ys(full_ys, theta_cut = jnp.pi):\n",
        "    # Applying a crude filter for chaotic trajectories\n",
        "    mask = jnp.any((abs(full_ys['theta1']) > theta_cut) | (abs(full_ys['theta2']) > theta_cut), axis=1)\n",
        "    nonchaotic_ys = jax.tree.map(lambda m: m[~mask], full_ys)\n",
        "    chaotic_ys = jax.tree.map(lambda m: m[mask], full_ys)\n",
        "    return nonchaotic_ys, chaotic_ys\n",
        "\n",
        "training_ys, chaotic_ys = split_ys(full_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc8da95",
      "metadata": {
        "id": "0dc8da95"
      },
      "source": [
        "### Visualisation\n",
        "\n",
        "The trajectories of the double pendula can be visualised in some interesting ways. For example, the histogram below.\n",
        "\n",
        "These can be used to understand the dynamics, but also make very nice visuals!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b024a254",
      "metadata": {
        "id": "b024a254"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(dpi=200,figsize=(6,2.5))\n",
        "ax1 = fig.add_subplot(121)\n",
        "plt.hist2d(training_ys['theta1'].flatten(),training_ys['theta2'].flatten(),\n",
        "           bins=[jnp.linspace(-jnp.pi,jnp.pi,200),jnp.linspace(-jnp.pi,jnp.pi,200)],\n",
        "           vmax=500,cmap='cubehelix')\n",
        "plt.colorbar()\n",
        "plt.xlabel(r'$\\theta_1$')\n",
        "plt.ylabel(r'$\\theta_2$')\n",
        "plt.title('Non-Chaotic')\n",
        "\n",
        "ax2 = fig.add_subplot(122)\n",
        "plt.hist2d(chaotic_ys['theta1'].flatten(),chaotic_ys['theta2'].flatten(),\n",
        "           bins=[jnp.linspace(-jnp.pi,jnp.pi,200),jnp.linspace(-jnp.pi,jnp.pi,200)],\n",
        "           vmax=10,cmap='cubehelix')\n",
        "plt.colorbar()\n",
        "plt.xlabel(r'$\\theta_1$')\n",
        "plt.ylabel(r'$\\theta_2$')\n",
        "plt.title('Chaotic')\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11dabcea",
      "metadata": {
        "id": "11dabcea"
      },
      "source": [
        "### NDE implementation\n",
        "\n",
        "Below we will define our Neural Differential Equation module using jax and libraries (most notably equinox and diffrax).\n",
        "\n",
        "- [Diffrax Documentation](https://docs.kidger.site/diffrax/)\n",
        "- [Equinox Documentation](https://docs.kidger.site/equinox/)\n",
        "\n",
        "**We will use a Lagrangian network approach to deriving the equations of motion**\n",
        "\n",
        "- [Euler-Lagrange equation](https://en.wikipedia.org/wiki/Euler%E2%80%93Lagrange_equation#Statement)\n",
        "\n",
        "The Lagrangian approach is as follows,\n",
        "\n",
        "Firstly, we define a scalar function called the \"Lagrangian\", $\\mathcal{L}(\\theta_1,\\theta_2,\\dot{\\theta}_1,\\dot{\\theta}_2)$, which we will approximate using a scalar and a bilinear term:\n",
        "\n",
        "$$ \\mathcal{L}(\\theta_1,\\theta_2,\\dot{\\theta}_1,\\dot{\\theta}_2) = \\begin{bmatrix} \\dot{\\theta}_1 \\\\ \\dot{\\theta}_2 \\end{bmatrix}^T \\mathbf{M}(\\theta_1,\\theta_2) \\begin{bmatrix} \\dot{\\theta}_1 \\\\ \\dot{\\theta}_2 \\end{bmatrix} + \\mathcal{N}(\\theta_1,\\theta_2) $$\n",
        "\n",
        "Where $M$ is a positive definite matrix. $N$ will be determined from a neural network and expression for $M$ is provided from theory.\n",
        "\n",
        "Secondly, we can use AD to form the equations of motion from derivatives of the Lagrangian.\n",
        "\n",
        "This involves two Jacobian matrices:\n",
        "\n",
        "$$ J_{\\dot{\\theta}\\dot{\\theta}} = \\nabla_{\\dot{\\theta}} \\nabla_{\\dot{\\theta}}^T \\mathcal{L} $$\n",
        "$$ J_{\\theta\\dot{\\theta}} = \\nabla_{\\theta} \\nabla_{\\dot{\\theta}}^T \\mathcal{L} $$\n",
        "\n",
        "We then solve the following linear system for the required terms in our equations of motion (see introduction):\n",
        "\n",
        "$$ J_{\\dot{\\theta}\\dot{\\theta}} \\cdot \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial \\mathcal{L}}{\\partial \\theta_1} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial \\theta_2} \\end{bmatrix} - J_{\\theta\\dot{\\theta}} \\cdot \\begin{bmatrix} \\dot{\\theta_1} \\\\ \\dot{\\theta_2} \\end{bmatrix} $$\n",
        "\n",
        "Finally, we can use these equations of motion to solve for the trajectory of the system in time\n",
        "\n",
        "In the code block below, we set up this approach to NDEs for the double pendulum system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8058a90e",
      "metadata": {
        "id": "8058a90e"
      },
      "outputs": [],
      "source": [
        "class NeuralODE_Lagrangian(eqx.Module):\n",
        "    mlp_N: eqx.nn.MLP\n",
        "\n",
        "    # Neural ODE based on Lagrangian mechanics for the double pendulum\n",
        "    def __init__(self, width_size, depth, activation, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # To be completed\n",
        "        # Set up MLP\n",
        "        # Hint: no final bias aids stability and training\n",
        "        self.mlp_N =\n",
        "\n",
        "        # Initialize the final linear layer to zero\n",
        "        where = lambda m: m.layers[-1].weight\n",
        "        self.mlp_N = eqx.tree_at(where, self.mlp_N, jnp.zeros_like(self.mlp_N.layers[-1].weight))\n",
        "\n",
        "    def fourier_features(self, th):\n",
        "        # Convert angles to fourier features\n",
        "        # Embed periodicity into inputs explicity\n",
        "        return jnp.array([jnp.cos(th[0]), jnp.sin(th[0]), jnp.cos(th[1]), jnp.sin(th[1])])\n",
        "\n",
        "    def compute_mass_matrix(self, th):\n",
        "        # Ensures M is positive definite\n",
        "        x = jnp.cos(th[1]-th[0])\n",
        "        M = L**2*jnp.array([[1.0,0.5*x],[0.5*x,0.5]])\n",
        "        return M\n",
        "\n",
        "    def Lagrangian(self, th, tdot):\n",
        "        # Convert to fourier features for inputs\n",
        "        x_N = self.fourier_features(th)\n",
        "\n",
        "        # Add explicit gravity and length dependence\n",
        "        # This helps the NN generalise\n",
        "        N = g*L**2*self.mlp_N(x_N)\n",
        "\n",
        "        M = self.compute_mass_matrix(th)\n",
        "\n",
        "        return tdot.T @ (M @ tdot) + N\n",
        "\n",
        "    def grad_L(self, th, thdot):\n",
        "        \"\"\" Computes all the required gradients of the Lagrangian \"\"\"\n",
        "        # To be completed, define gradient functions here\n",
        "        grad_L_theta =\n",
        "        grad_L_thdot =\n",
        "\n",
        "        # Compute required Jacobians\n",
        "        J_ddot = jax.jacrev(grad_L_thdot,argnums=1)(th,thdot)\n",
        "        J_dot = jax.jacrev(grad_L_thdot,argnums=0)(th,thdot)\n",
        "\n",
        "        grad_L_theta = grad_L_theta(th,thdot)\n",
        "\n",
        "        return grad_L_theta, J_ddot, J_dot\n",
        "\n",
        "    def __call__(self, t, y, args):\n",
        "        theta_dot_array = jnp.array([y['theta1dot'], y['theta2dot']])\n",
        "        theta_array = jnp.array([y['theta1'], y['theta2']])\n",
        "\n",
        "        grad_L_theta, J_ddot, J_dot = self.grad_L(theta_array, theta_dot_array)\n",
        "\n",
        "        # To be completed\n",
        "        # Compute Right Hand Side of equations of motion\n",
        "        # RHS = dL/dtheta - J_{theta,theta_dot} @ theta_dot\n",
        "        RHS =\n",
        "\n",
        "        # Solve for angular accelerations\n",
        "        hs = jnp.linalg.solve(J_ddot, RHS)\n",
        "\n",
        "        return {\n",
        "        'theta1' : y['theta1dot'],\n",
        "        'theta2' : y['theta2dot'],\n",
        "        'theta1dot' : hs[0],\n",
        "        'theta2dot' : hs[1],\n",
        "        }\n",
        "\n",
        "class NeuralODE(eqx.Module):\n",
        "    func: NeuralODE_Lagrangian\n",
        "\n",
        "    def __init__(self, width_size, depth, activation, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.func = NeuralODE_Lagrangian(width_size, depth, activation, key=key)\n",
        "\n",
        "    def __call__(self, ts, y0, args):\n",
        "        \"\"\"\n",
        "        Similar to our examples in exercise, we set up diffeqsolve\n",
        "        but now our ODETerm uses our NDE equinox Module.\n",
        "        \"\"\"\n",
        "        # Set up and solve ODE\n",
        "        solution = diffrax.diffeqsolve(\n",
        "            diffrax.ODETerm(self.func),\n",
        "            diffrax.Tsit5(),\n",
        "            t0=ts[0],\n",
        "            t1=ts[-1],\n",
        "            dt0=ts[1] - ts[0],\n",
        "            y0=y0,\n",
        "            args=args,\n",
        "            stepsize_controller=diffrax.PIDController(rtol=args['rtol'], atol=args['atol']),\n",
        "            saveat=diffrax.SaveAt(ts=ts),\n",
        "            max_steps=int(1e6)\n",
        "        )\n",
        "        return solution.ys"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40bfa229",
      "metadata": {
        "id": "40bfa229"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Here we define a function to perform training on our NDE model based on input training data. This training function involves 3 key strategies:\n",
        "\n",
        "1. Batching of NDE solves over training data\n",
        "2. A stepwise learning rate schedule\n",
        "3. A stepwise training data truncation schedule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83029ae6",
      "metadata": {
        "id": "83029ae6"
      },
      "outputs": [],
      "source": [
        "from jax.flatten_util import ravel_pytree\n",
        "\n",
        "def MSE_pytrees(pred,truth):\n",
        "    return jnp.mean((jax.flatten_util.ravel_pytree(pred)[0]-jax.flatten_util.ravel_pytree(truth)[0])**2)\n",
        "\n",
        "def train_DPNDE(ts,ys,args,\n",
        "    model,\n",
        "    optimiser,\n",
        "    lr_strategy,\n",
        "    steps_strategy,\n",
        "    length_strategy,\n",
        "    grad_clip,\n",
        "    batch_size,\n",
        "    print_every=50,\n",
        "    reset_state=True,\n",
        "):\n",
        "    @eqx.filter_value_and_grad\n",
        "    def grad_loss(model, ti, yi):\n",
        "        \"\"\"\n",
        "        Compute loss and gradients\n",
        "        Total loss is MSE with respect to both trajectories and equations of motion\n",
        "        \"\"\"\n",
        "        # Handle batch\n",
        "        batched_model = jax.vmap(model,in_axes=(None,0,None))\n",
        "        y0 = jax.tree.map(lambda v : v[:,0], yi)\n",
        "        y_pred = batched_model(ti, y0, args)\n",
        "        # Error with respect to trajectories\n",
        "        MSE = MSE_pytrees(y_pred,yi)\n",
        "\n",
        "        # Batched Equations of Motion (EoM)\n",
        "        EoM_pred = lambda y : jax.vmap(model.func,in_axes=(0,0,None))(ti,y,args)\n",
        "        batched_EoM_pred = jax.vmap(EoM_pred,in_axes=0)(y_pred)\n",
        "        EoM_truth = lambda y : jax.vmap(DPS.DP_equations,in_axes=(0,0,None))(ti,y,{'L' : L})\n",
        "        batched_EoM_truth = jax.vmap(EoM_truth,in_axes=0)(y_pred)\n",
        "        # Error with respect to equations of motion\n",
        "        MEoM = MSE_pytrees(batched_EoM_pred,batched_EoM_truth)\n",
        "\n",
        "        return MSE + MEoM\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def make_step(ti, yi, model, opt_state):\n",
        "        loss, grads = grad_loss(model, ti, yi)\n",
        "        flat_grad, _ = ravel_pytree(grads)\n",
        "        grad_norm = jnp.linalg.norm(flat_grad)\n",
        "\n",
        "        # To be completed\n",
        "        # Compute parameter updates and apply to model\n",
        "        # See NDE example in exercise\n",
        "        updates, opt_state =\n",
        "        model =\n",
        "\n",
        "        return loss, grad_norm, model, opt_state\n",
        "\n",
        "    history = {'loss' : [], 'grad_norm' : []}\n",
        "    count = 0\n",
        "    batch_key = jax.random.PRNGKey(5445)\n",
        "\n",
        "    # Set up optimiser, gradient clipping and learning rate schedule\n",
        "    if(len(lr_strategy) > 1):\n",
        "        lr_schedule = optax.schedules.piecewise_constant_schedule(lr_strategy[0],{k:v for k,v in zip(steps_strategy[:-1],lr_strategy[1:])})\n",
        "        optim = optax.chain(\n",
        "            optax.clip_by_global_norm(grad_clip),\n",
        "            optimiser(learning_rate=lr_schedule)\n",
        "        )\n",
        "    else:\n",
        "        optim = optax.chain(\n",
        "            optax.clip_by_global_norm(grad_clip),\n",
        "            optimiser(learning_rate=lr_strategy[0])\n",
        "        )\n",
        "\n",
        "    # Set up optimiser state\n",
        "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "\n",
        "    length_size = ts.shape[0]\n",
        "    length_prev = 0\n",
        "    for steps, length in zip(steps_strategy, length_strategy):\n",
        "        if reset_state and length_prev != length:\n",
        "            opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "        length_prev = 1.0*length\n",
        "\n",
        "        # To be completed\n",
        "        # Find index respective to length\n",
        "        Nts =\n",
        "\n",
        "        _ts = ts[:Nts]\n",
        "        for step in range(steps):\n",
        "            batch_key, _ = jax.random.split(batch_key)\n",
        "            bidx = jax.random.choice(batch_key, jnp.arange(ys['theta1'].shape[0]), shape=(batch_size,), replace=False)\n",
        "            _ys = jax.tree.map(lambda v : v[bidx,:Nts], ys)\n",
        "\n",
        "            count += 1\n",
        "            start = time.time()\n",
        "            loss, grad_norm, model, opt_state = make_step(_ts, _ys, model, opt_state)\n",
        "            history['loss'].append(loss)\n",
        "            history['grad_norm'].append(grad_norm)\n",
        "            end = time.time()\n",
        "            if (step % print_every) == 0 or step == steps - 1:\n",
        "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22f4f902",
      "metadata": {
        "id": "22f4f902"
      },
      "source": [
        "Here we can define our base model and learning strategy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c60455",
      "metadata": {
        "id": "d1c60455"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "# To be chosen, only a small network is needed, depth ~ 3\n",
        "WIDTH_SIZE =\n",
        "DEPTH =\n",
        "# Consider smooth activations, tanh, swish, gelu, etc.\n",
        "ACT_FUNC =\n",
        "\n",
        "base_model = NeuralODE(width_size=WIDTH_SIZE, depth=DEPTH, activation=ACT_FUNC , key=jax.random.PRNGKey(420))\n",
        "\n",
        "trained_model = copy.deepcopy(base_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a3dfde6",
      "metadata": {
        "id": "0a3dfde6"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "optimiser = optax.adam\n",
        "# Gradient clipping value, learning rates, steps and lengths to be chosen\n",
        "# To check gradient clipping value, check grad_norm returned during training (in history)\n",
        "grad_clip = 1e30 # Arbitrarily large value\n",
        "\n",
        "lr_strategy = [5e-3,2.5e-3,1e-3]\n",
        "steps_strategy = [300,500,500]\n",
        "length_strategy = [0.5,0.75,1.0]\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d66f508c",
      "metadata": {
        "id": "d66f508c"
      },
      "source": [
        "## Problems\n",
        "\n",
        "- Try training a model on a single training data example\n",
        "    - Consider: How the model trains, what complexity of network is needed, short term vs long term prediction\n",
        "    - What happens if you try to train on full time series (length_strategy = 1.0) from epoch 0?\n",
        "- Perform a test-train (e.g. roughly 80/20) split and train the model and evaluate its performance.\n",
        "- Can you improve this model via modifying the hyper parameters?\n",
        "\n",
        "_Warning: Training can take a long time for large training sets (~100s of ms per step for batch_size=32), experiment on smaller sections of data to understand NDE behaviours before attempting more extensive training._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16bbe8e6",
      "metadata": {
        "id": "16bbe8e6"
      },
      "outputs": [],
      "source": [
        "split_idx = 1\n",
        "\n",
        "training_data = jax.tree.map(lambda v : v[:split_idx,:], training_ys)\n",
        "\n",
        "trained_model, history = train_DPNDE(\n",
        "    ts,training_data,NDE_args,trained_model,\n",
        "    optimiser,lr_strategy,steps_strategy,length_strategy,\n",
        "    grad_clip,batch_size,print_every=50,reset_state=False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfd753d0",
      "metadata": {
        "id": "bfd753d0"
      },
      "source": [
        "### Saving your model\n",
        "\n",
        "Code below can be used to save/serialise your trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9475042f",
      "metadata": {
        "id": "9475042f"
      },
      "outputs": [],
      "source": [
        "eqx.tree_serialise_leaves(f\"DPNDE_W{WIDTH_SIZE}_D{DEPTH}_act{str(ACT_FUNC).split(' ')[-3]}_trained.eqx\", trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "818cd9df",
      "metadata": {
        "id": "818cd9df"
      },
      "source": [
        "### Plotting loss history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712cddc5",
      "metadata": {
        "id": "712cddc5"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(dpi=200)\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax2 = fig.add_subplot(212)\n",
        "\n",
        "ax1.semilogy(history['loss'])\n",
        "smoothed_loss = jnp.convolve(jnp.array(history['loss']),jnp.ones(batch_size)/batch_size,mode='valid')\n",
        "ax1.semilogy(smoothed_loss)\n",
        "ax2.semilogy(history['grad_norm'])\n",
        "ax1.set_ylabel('Loss')\n",
        "ax2.set_ylabel('Gradient Norm')\n",
        "ax2.set_xlabel('Training Step')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6a18da",
      "metadata": {
        "id": "fb6a18da"
      },
      "source": [
        "Use the below to compute a MSE on a trained model using test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b682f2fc",
      "metadata": {
        "id": "b682f2fc"
      },
      "outputs": [],
      "source": [
        "def compute_loss_from_trained_model(model,ts,test_data):\n",
        "    batched_model = jax.vmap(model,in_axes=(None,0,None))\n",
        "    y0 = jax.tree.map(lambda v : v[:,0], test_data)\n",
        "    y_pred = batched_model(ts,y0,NDE_args)\n",
        "    return MSE_pytrees(y_pred,test_data)\n",
        "\n",
        "test_data = jax.tree.map(lambda v : v[split_idx:,:], training_ys)\n",
        "\n",
        "print(compute_loss_from_trained_model(trained_model,ts,test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095334dd",
      "metadata": {
        "id": "095334dd"
      },
      "source": [
        "Use below to plot trajectories for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f76d109",
      "metadata": {
        "id": "1f76d109"
      },
      "outputs": [],
      "source": [
        "test_data = jax.tree.map(lambda v : v[2:3,:], training_ys)\n",
        "\n",
        "plt.plot(test_data['theta1'].T)\n",
        "plt.plot(test_data['theta2'].T)\n",
        "y0 = jax.tree.map(lambda v : v[0,0], test_data)\n",
        "y_pred = trained_model(ts,y0,NDE_args)\n",
        "\n",
        "plt.plot(y_pred['theta1'],c='k',ls='--')\n",
        "plt.plot(y_pred['theta2'],c='k',ls=':')\n",
        "\n",
        "plt.ylim(-2*jnp.pi,2*jnp.pi)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "104ec0ed",
      "metadata": {
        "id": "104ec0ed"
      },
      "source": [
        "### Loading pre-trained model\n",
        "\n",
        "Code above can save your model such that you can re-load it later. Here is how you might load from the saved model file.\n",
        "\n",
        "```python\n",
        "\n",
        "model_original = NeuralODE(in_size=IN_SIZE, out_size=OUT_SIZE, width_size=WIDTH_SIZE, depth=DEPTH, activation=ACT_FUNC , key=jax.random.PRNGKey(420))\n",
        "# Note that all sizes and depths must match between model_original and model_loaded\n",
        "model_loaded = eqx.tree_deserialise_leaves(\"<model_to_be_loaded>.eqx\", model_original)\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ed8b1c2",
      "metadata": {
        "id": "0ed8b1c2"
      },
      "source": [
        "### Chaotic trajectories\n",
        "\n",
        "For some values of initial conditions, the double pendulum can show chaotic trajectories. In other words, small perturbations to the initial conditions lead to large variation in trajectories. Let us look at a specific example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ab04fd",
      "metadata": {
        "id": "a4ab04fd"
      },
      "outputs": [],
      "source": [
        "ts_chaos = jnp.linspace(0.0,20.0,2000)\n",
        "\n",
        "DP_chaos = DPS(ts=ts_chaos)\n",
        "\n",
        "args = {'L' : 1.0}\n",
        "\n",
        "init_cond = {\n",
        "    'theta1' : 1.0*jnp.pi/3.0,\n",
        "    'theta2' : 1.0*jnp.pi/3.0,\n",
        "    'theta1dot' : 0.0,\n",
        "    'theta2dot' : 0.0,\n",
        "    }\n",
        "\n",
        "init_cond_perturb = {k : (1+1e-5)*v for k,v in init_cond.items()}\n",
        "\n",
        "ys_nochaos_1 = DP_chaos(init_cond, args)\n",
        "ys_nochaos_2 = DP_chaos(init_cond_perturb, args)\n",
        "\n",
        "init_cond = {\n",
        "    'theta1' : 2.0*jnp.pi/3.0,\n",
        "    'theta2' : 1.0*jnp.pi/3.0,\n",
        "    'theta1dot' : 0.0,\n",
        "    'theta2dot' : 0.0,\n",
        "    }\n",
        "\n",
        "init_cond_perturb = {k : (1+1e-5)*v for k,v in init_cond.items()}\n",
        "\n",
        "ys_chaos_1 = DP_chaos(init_cond, args)\n",
        "ys_chaos_2 = DP_chaos(init_cond_perturb, args)\n",
        "\n",
        "fig = plt.figure(dpi=200)\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax2 = fig.add_subplot(212,sharex=ax1)\n",
        "\n",
        "ax1t = ax1.twinx()\n",
        "ax2t = ax2.twinx()\n",
        "\n",
        "ax1.plot(ts_chaos,ys_nochaos_1['theta1'],label='Theta 1 - Unperturbed',color='blue')\n",
        "ax1.plot(ts_chaos,ys_nochaos_2['theta1'],label='Theta 1 - Perturbed',color='orange',ls='--')\n",
        "\n",
        "ax1t.plot(ts_chaos,ys_nochaos_1['theta2'],label='Theta 2 - Unperturbed',color='red')\n",
        "ax1t.plot(ts_chaos,ys_nochaos_2['theta2'],label='Theta 2 - Perturbed',color='green',ls='--')\n",
        "\n",
        "ax2.plot(ts_chaos,ys_chaos_1['theta1'],label='Theta 1 - Unperturbed',color='blue')\n",
        "ax2.plot(ts_chaos,ys_chaos_2['theta1'],label='Theta 1 - Perturbed',color='orange',ls='--')\n",
        "\n",
        "ax2t.plot(ts_chaos,ys_chaos_1['theta2'],label='Theta 2 - Unperturbed',color='red')\n",
        "ax2t.plot(ts_chaos,ys_chaos_2['theta2'],label='Theta 2 - Perturbed',color='green',ls='--')\n",
        "\n",
        "ax1.set_title('Non-Chaotic Trajectories')\n",
        "ax2.set_title('Chaotic Trajectories')\n",
        "\n",
        "ax1.set_ylabel(r'$\\theta_1$')\n",
        "ax2.set_ylabel(r'$\\theta_1$')\n",
        "\n",
        "ax1t.set_ylabel(r'$\\theta_2$')\n",
        "ax2t.set_ylabel(r'$\\theta_2$')\n",
        "\n",
        "ax1.set_xlim([ts_chaos[0],ts_chaos[-1]])\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f2923f3",
      "metadata": {
        "id": "7f2923f3"
      },
      "source": [
        "## Problems:\n",
        "\n",
        "- How does your trained model perform on a chaotic trajectory (select a number from the chaotic_ys paritioned at the beginning of the project)? Why is the performance likely to be poor?\n",
        "- What issue will likely arise if a model is only trained on a short timeframe (e.g. [0,10]) given what is shown in the example above?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f7b9c7d",
      "metadata": {
        "id": "9f7b9c7d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "526bfa54",
      "metadata": {
        "id": "526bfa54"
      },
      "source": [
        "## Extension problems:\n",
        "\n",
        "- Include the chaotic trajectories in your training data and see how the NDE model trains.\n",
        "- What implications does this have more ML models of chaotic systems e.g. weather?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abb30674",
      "metadata": {
        "id": "abb30674"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lagradept_update",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}