{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/AIMSLecture/blob/main/2026/Project_TFPINN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323beb5a",
      "metadata": {
        "id": "323beb5a"
      },
      "source": [
        "# Physics Informed Neural Networks for Non-Linear Poisson Equation (Thomas Fermi Equation)\n",
        "\n",
        "\n",
        "The Thomas-Fermi equation describes a compressed neutral atom semi-classically and is derived from Poisson's equation. It takes the form a second order, non-linear ordinary differential equation:\n",
        "\n",
        "$$ \\frac{d^2 y}{d x^2} = \\frac{(y x_0)^{\\frac{3}{2}}}{x^{\\frac{1}{2}}} $$\n",
        "\n",
        "Subject to the following boundary conditions:\n",
        "\n",
        "$$ y(0) = 1 $$\n",
        "$$ \\frac{dy}{dx}(1) = y(1) $$\n",
        "\n",
        "Where $x_0$ is a constant (the radius of the \"ion-sphere\").\n",
        "\n",
        "We are interested in a PINN solution to the Thomas-Fermi equation.\n",
        "\n",
        "First, we consider how to construct a PDE loss term.\n",
        "\n",
        "It is clear from the Thomas-Fermi equation and the boundary condition at $x = 0$, that the second derivative of $y$ diverges at $x = 0$. Therefore, we choose a loss function to minimise the contribution from points approaching $x = 0$:\n",
        "\n",
        "$$ \\mathcal{L}(y,y'',x,x_0) = \\left(\\frac{x^{\\frac{1}{2}+m}}{x_0^{\\frac{3}{2}}}\\frac{d^2 y}{d x^2}  - x^m y^{\\frac{3}{2}}\\right)^2 $$\n",
        "\n",
        "Where $m$ is some positive real number, e.g. $\\frac{1}{2}$.\n",
        "\n",
        "Next, we consider how we can enforce the boundary conditions by construction - rather than including them as loss terms.\n",
        "\n",
        "To do this, we consider transformations applied to our PINN which enforce the boundary conditions by construction. A sensible choice is of the form:\n",
        "\n",
        "$$ w(x, x_0) = A(x) + B(x) \\mathcal{N} + C(x) \\mathcal{N}'$$\n",
        "$$ y(x, x_0) = t(w) $$\n",
        "\n",
        "Where $t(x)$ is some non-linear function to maintain positivity (we will use $\\exp(x)$). A, B and C are polynomial functions of $x$ only - these can be derived from the boundary conditions. $\\mathcal{N}$ and $\\mathcal{N}'$ are the PINN and its derivative w.r.t. $x$.\n",
        "\n",
        "Next, we know that the leading order term of the solution as we approach $x = 0$ is as follows:\n",
        "\n",
        "$$ y(x) \\approx 1 + \\alpha_1 x + \\alpha_2 x^{3/2} + \\mathcal{O}(x^2) $$\n",
        "\n",
        "Where $\\alpha_i$ are unknown constants. Therefore, we will provide other powers of $x$ to the inputs of the PINN to include this behaviour.\n",
        "\n",
        "Finally, we know that the boundary value of $y(1)$ is a function of $x_0$ only, and therefore we make our PINN model a product of models:\n",
        "\n",
        "$$ \\mathcal{N}(x,x_0) = \\mathcal{N}_{PINN} (x^{3/2},x,x_0) \\mathcal{N}_{BC}(x_0) $$\n",
        "\n",
        "In the following project, you will implement the above PINN solution and explore its behaviour.\n",
        "\n",
        "_Thanks to Guzman Sanchez Gonzalez for his work on PINNs for Thomas-Fermi which acted as basis for this project._\n",
        "\n",
        "## Problem statement:\n",
        "\n",
        "Your task is to create a PINN capable of solving the Thomas-Fermi equation for a range of $x_0$ values.\n",
        "\n",
        "- We must derive suitable functions for $A(x)$, $B(x)$ and $C(x)$ to respect the boundary conditions\n",
        "- We must construct suitable NN architectures for the PINN and BC components\n",
        "- We must create a training method to train the PINN on varied $x$ and $x_0$ values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512afae1",
      "metadata": {
        "id": "512afae1"
      },
      "outputs": [],
      "source": [
        "!pip install optax equinox\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as jnn\n",
        "import equinox as eqx\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deriving the boundary conditions\n",
        "\n",
        "We wish to derive suitable functions that enforce our boundary conditions. To do this we use the following:\n",
        "\n",
        "$$ w(x, x_0) = A(x) + B(x) \\mathcal{N} + C(x) \\mathcal{N}'$$\n",
        "$$ y(x, x_0) = \\exp(w) $$\n",
        "\n",
        "With boundary conditions:\n",
        "\n",
        "$$ y(0) = 1 $$\n",
        "$$ \\frac{dy}{dx}(1) = y(1) $$\n",
        "\n",
        "For any $x_0$. Which for $w$ are:\n",
        "\n",
        "$$ w(0) = 0 $$\n",
        "$$ \\frac{dw}{dx} = 1 $$\n",
        "\n",
        "These lead to the following constraints on A, B and C:\n",
        "\n",
        "$$ A(0) = 0, \\ B(0) = 0, \\ C(0) = 0 $$\n",
        "$$ A'(1) = 1, \\ B'(1) = 0, \\ B(1) = -C'(1), \\ C(1) = 0 $$\n",
        "\n",
        "A suitable set of functions which statisfy all these constraints are:\n",
        "\n",
        "$$ A(x) = \\frac{1}{2}x^2 $$\n",
        "$$ B(x) = 1 - (1 - x)^2 $$\n",
        "$$ C(x) = x (1 - x) $$"
      ],
      "metadata": {
        "id": "5H4KEV7F7iHJ"
      },
      "id": "5H4KEV7F7iHJ"
    },
    {
      "cell_type": "markdown",
      "id": "bfcad590",
      "metadata": {
        "id": "bfcad590"
      },
      "source": [
        "### PINN model\n",
        "\n",
        "Below you will complete the following PINN model implemented using jax and equniox. Reference the equinox docs (and course materials) for NN implementations:\n",
        "\n",
        "- [Equinox Documentation](https://docs.kidger.site/equinox/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb71b17",
      "metadata": {
        "id": "ebb71b17"
      },
      "outputs": [],
      "source": [
        "class TFPINNModel(eqx.Module):\n",
        "    mlp_PINN: eqx.nn.MLP\n",
        "    mlp_BC: eqx.nn.MLP\n",
        "    m: float\n",
        "\n",
        "    def __init__(self, width_size, depth, m, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.m = m\n",
        "        key, subkey = jax.random.split(key)\n",
        "\n",
        "        # Create optimal NN architecture\n",
        "        # To be completed as exercise\n",
        "        # Use eqx MLP like in tutorial\n",
        "        base_model =\n",
        "        # Initialize the final linear layer to zero\n",
        "        where = lambda m: m.layers[-1].weight\n",
        "        self.mlp_PINN = eqx.tree_at(where, base_model, jnp.zeros_like(base_model.layers[-1].weight))\n",
        "\n",
        "        key, subkey = jax.random.split(key)\n",
        "        # To be completed as exercise\n",
        "        # Use eqx MLP like in tutorial\n",
        "        self.mlp_BC =\n",
        "\n",
        "    def boundary_condition_constraints(self,x):\n",
        "        # To be completed as exercise\n",
        "        A =\n",
        "        B =\n",
        "        C =\n",
        "        return A, B, C\n",
        "\n",
        "    def power_input(self, x, x0):\n",
        "        return jnp.sqrt(x)*x\n",
        "\n",
        "    def PINN(self, x, x0):\n",
        "        return self.mlp_PINN(jnp.array([self.power_input(x,x0), x, jnp.log10(x0)]))*self.mlp_BC(jnp.log10(x0))\n",
        "\n",
        "    def dPINN(self, x, x0):\n",
        "        # To be completed as exercise\n",
        "        # Compute grad of PINN\n",
        "        dPINNdx =\n",
        "        return dPINNdx\n",
        "\n",
        "    def y_NN(self, x, x0):\n",
        "        A, B, C = self.boundary_condition_constraints(x)\n",
        "        N = self.PINN(x, x0)\n",
        "        Nprime = self.dPINN(x, x0)\n",
        "        w = A + B * N + C * Nprime\n",
        "        # Choose suitable transformation of w to ensure positivity, e.g. exponential\n",
        "        # This needs to be consistent with the boundary conditions\n",
        "        # To be completed as exercise\n",
        "        y = # Some function of w\n",
        "        return y\n",
        "\n",
        "    def y(self, x, x0):\n",
        "        return self.y_NN(x, x0)\n",
        "\n",
        "    def dydx(self, x, x0):\n",
        "        return jax.grad(self.y)(x, x0)\n",
        "\n",
        "    def d2ydx2(self, x, x0):\n",
        "        return jax.grad(jax.grad(self.y))(x, x0)\n",
        "\n",
        "    def __call__(self, x, x0):\n",
        "        return eqx.filter_jit(eqx.filter_vmap(self.y))(x,x0)\n",
        "\n",
        "    def residual_loss(self, x, x0):\n",
        "        # To be completed as exercise\n",
        "        loss =\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84ebd23",
      "metadata": {
        "id": "c84ebd23"
      },
      "source": [
        "### Training data creation\n",
        "\n",
        "Training data simply consists of randomly sampled $x$ and $x_0$ values. Below is a simple implementation that performs uniform sampling - however other sampling schemes can be considered."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "934ec079",
      "metadata": {
        "id": "934ec079"
      },
      "outputs": [],
      "source": [
        "def create_training_data(Nsamples,x0_range,x_lower=1e-3, key = jax.random.PRNGKey(42)):\n",
        "    key,subkey = jax.random.split(key)\n",
        "    xs = x_lower + (1.0-x_lower)*jax.random.uniform(subkey,shape=(Nsamples,))\n",
        "    key,subkey = jax.random.split(key)\n",
        "    x0s = x0_range[0] + (x0_range[1]-x0_range[0])*jax.random.uniform(subkey,shape=(Nsamples,))\n",
        "    return xs, x0s"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f71e2637",
      "metadata": {
        "id": "f71e2637"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Below you should complete the training loop for the PINN model which makes use of jax and optax - make refernece to the course materials and optax documentation.\n",
        "\n",
        "- [Optax Documentation](https://optax.readthedocs.io/en/latest/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38220a90",
      "metadata": {
        "id": "38220a90"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "from jax.flatten_util import ravel_pytree\n",
        "\n",
        "def train_TFPINN(TFPINN, xs, x0s, batch_size, num_epochs, learning_rate, key = jax.random.PRNGKey(42), print_every=10):\n",
        "    optimizer = optax.adam(learning_rate)\n",
        "    opt_state = optimizer.init(eqx.filter(TFPINN, eqx.is_inexact_array))\n",
        "\n",
        "    def batch_loss_fn(TFPINN, xs, x0s):\n",
        "        # Compute the mean residual loss over the batch\n",
        "        batched_loss = eqx.filter_vmap(TFPINN.residual_loss)\n",
        "        loss = jnp.mean(batched_loss(xs, x0s))\n",
        "        return loss\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def update(TFPINN, opt_state, batch_xs, batch_x0s):\n",
        "        # Compute loss and gradients\n",
        "        loss, grads = eqx.filter_value_and_grad(batch_loss_fn)(TFPINN, batch_xs, batch_x0s)\n",
        "        flat_grad, _ = ravel_pytree(grads)\n",
        "        grad_norm = jnp.linalg.norm(flat_grad)\n",
        "        updates, opt_state = optimizer.update(grads, opt_state)\n",
        "        TFPINN = eqx.apply_updates(TFPINN, updates)\n",
        "        return TFPINN, opt_state, loss, grad_norm\n",
        "\n",
        "    history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = []\n",
        "        grad_norm = []\n",
        "        for _ in range(len(xs)//batch_size):\n",
        "            # Set up batching, to be completed\n",
        "            # Look at jax.random.choice for help\n",
        "            key, subkey = jax.random.split(key)\n",
        "            batch_xs =\n",
        "            key, subkey = jax.random.split(key)\n",
        "            batch_x0s =\n",
        "            TFPINN, opt_state, _loss, _grad_norm = update(TFPINN, opt_state, batch_xs, batch_x0s)\n",
        "            loss.append(_loss)\n",
        "            grad_norm.append(_grad_norm)\n",
        "        loss = jnp.mean(jnp.array(loss))\n",
        "        grad_norm = jnp.mean(jnp.array(grad_norm))\n",
        "        if epoch % print_every == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {loss}, Grad Norm: {grad_norm}\")\n",
        "        history.append(loss)\n",
        "\n",
        "    return TFPINN, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d65b5f9",
      "metadata": {
        "id": "2d65b5f9"
      },
      "source": [
        "## Problems\n",
        "\n",
        "- Train a PINN for $x_0 = 1$, what is the shape of the solution as function of $x$ and what is value at $x = 1$? What is the lowest loss value obtained?\n",
        "- Train a PINN for $x_0$ in range $[0.5,5]$, how does the solution shape change with $x_0$? What is the lowest loss value obtained?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e417bdb",
      "metadata": {
        "id": "4e417bdb"
      },
      "outputs": [],
      "source": [
        "# Training data hyperparameters\n",
        "Nsamples = # Number of samples to create, e.g. int(1e6)\n",
        "x0_range = # List of two floats defining the range of x0 values\n",
        "\n",
        "xs, x0s = create_training_data(Nsamples,x0_range,x_lower=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c008fd8f",
      "metadata": {
        "id": "c008fd8f"
      },
      "outputs": [],
      "source": [
        "# Model hyperparameters\n",
        "# Pick suitable values (e.g. WIDTH_SIZE=16, DEPTH=3, M_LOSS=0.5)\n",
        "WIDTH_SIZE =\n",
        "DEPTH =\n",
        "M_LOSS =\n",
        "\n",
        "TFPINN = TFPINNModel(width_size=WIDTH_SIZE, depth=DEPTH, m=M_LOSS, key=jax.random.PRNGKey(1234))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f1f8d89",
      "metadata": {
        "id": "3f1f8d89"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "# Note model evaluation is very cheap so batch_size can be large, with right model hyperparameters training will be very fast\n",
        "BATCH_SIZE =\n",
        "NUM_EPOCHS =\n",
        "LEARNING_RATE =\n",
        "\n",
        "TFPINN, history = train_TFPINN(TFPINN, xs, x0s, BATCH_SIZE, NUM_EPOCHS, LEARNING_RATE, print_every=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c84e603",
      "metadata": {
        "id": "7c84e603"
      },
      "source": [
        "## Problem:\n",
        "- What pathological solutions can the PINN fall into? Try a large value of $m$ to find these. Why does this occur?\n",
        "- Can you forsee an issue with large $x_0$ values? e.g. 10000?\n",
        "\n",
        "## Extension problems:\n",
        "\n",
        "- Can you train over a larger range of $x_0$ values? What changes to the model/training/data need to be made?\n",
        "- (Difficult) The Thomas-Fermi-Dirac equation in an extension to Thomas-Fermi theory. The same boundary conditions apply but the equation is modified to the following:\n",
        "\n",
        "$$ \\frac{d^2 y}{d x^2} =  x \\left(\\epsilon x_0 + \\sqrt{\\frac{y x_0}{x}}\\right)^3$$\n",
        "\n",
        "Where $\\epsilon$ is an additional constant (with value $\\lesssim 0.2$). What modifications need to be made to the model/training/data? Show results for a range of $x_0$ and $\\epsilon$ values."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lagradept_update",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}